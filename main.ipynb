{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "c30cfb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import json, re, random, math\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "import os\n",
    "import re\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import retry\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from openai import OpenAI\n",
    "from contextualbandits.online import LinUCB\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_PATH = \"artifacts/data/sanitized-sample.json\"\n",
    "PREPROCESSED_DATA_PATH = \"artifacts/data/preprocessed-sanitized-sample.json\"\n",
    "\n",
    "CACHE_PATH = \"artifacts/cache/\"\n",
    "FEATURES_CSV_PATH = \"artifacts/features.csv\"\n",
    "FEATURES_JSON_PATH = \"artifacts/features.json\"\n",
    "COACHING_CARDS_JSON = \"artifacts/coaching_next.json\"\n",
    "REPORT_JSON_PATH = \"artifacts/report.json\"\n",
    "\n",
    "EXTRACT_LLM_FEATURES_SYSTEM_PROMPT_PATH = \"artifacts/system_prompts/extract_llm_features.txt\"\n",
    "COACHING_CARD_GENERATION_SYSTEM_PROMPT_PATH = \"artifacts/system_prompts/generate_coaching_card.txt\"\n",
    "\n",
    "ACTIONS = [\"clarity\", \"active_listening\", \"call_to_action\", \"friendliness\"]\n",
    "\n",
    "DEFAULT_LINUCB_ALPHA = 0.4\n",
    "DEFAULT_SKILL_WEIGHT = 0.6\n",
    "DEFAULT_OVERALL_WEIGHT = 0.4\n",
    "\n",
    "DEFAULT_LLM_MODEL = \"gpt-4.1-nano\"\n",
    "DEFAULT_TEMPERATURE = 0.01\n",
    "FORCE_OVERWRITE_CACHE = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "cb0178bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "\n",
    "def preprocess_dataset(input_path: str=DATA_PATH, output_path: str=PREPROCESSED_DATA_PATH) -> None:\n",
    "    with open(input_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "        dataset = json.load(f)\n",
    "    dataset = sorted(dataset, key=lambda x: parse_timestamp(x.get(\"start_time\",\"1970-01-01T00:00:00Z\")))\n",
    "\n",
    "    mapping = {\n",
    "        \"clarity and enthusiasm in pitch\":\"clarity\",\n",
    "        \"active listening and objection handling\":\"active_listening\",\n",
    "        \"effective call to action\":\"call_to_action\",\n",
    "        \"friendliness and respectful tone\":\"friendliness\"\n",
    "    }\n",
    "    for i in range(len(dataset)):\n",
    "        session = dataset[i]\n",
    "        criteria = session.get(\"assessment_data\",{}).get(\"criteria\",{})\n",
    "        for k,v in mapping.items():\n",
    "            # score = criteria.get(k,{}).get(\"score\", np.nan)\n",
    "            session[\"assessment_data\"][\"criteria\"][v] = criteria.get(k,{})\n",
    "            session[\"assessment_data\"][\"criteria\"].pop(k, None)\n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(dataset, f, indent=2)\n",
    "\n",
    "\n",
    "def parse_timestamp(ts: str) -> datetime:\n",
    "    try:\n",
    "        return datetime.fromisoformat(ts.replace(\"Z\",\"+00:00\"))\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Failed to parse timestamp {ts}: {e}\")\n",
    "        return datetime.strptime(ts.split(\".\")[0], \"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "def parse_transcript_to_string(transcript: List[Dict[str,str]]) -> str:\n",
    "    transcript_str = \"\"\n",
    "    for t in transcript:\n",
    "        role = t.get(\"type\",\"\").lower()\n",
    "        if \"agent_ai\" in role:\n",
    "            prefix = \"AI Agent: \"\n",
    "        elif \"user\" in role:\n",
    "            prefix = \"User: \"\n",
    "        else:\n",
    "            prefix = \"Unknown: \"\n",
    "            logger.warning(f\"Unknown role in transcript: {role}\")\n",
    "        transcript_str += f\"{prefix}{t.get('data','')}\\n\"\n",
    "    return transcript_str\n",
    "\n",
    "def extract_json(text: str) -> Any:\n",
    "    json_pattern = r\"```json(.*?)```\"\n",
    "    match = re.search(json_pattern, text, re.DOTALL)\n",
    "    if match:\n",
    "        json_str = match.group(1).strip()\n",
    "        try:\n",
    "            return json.loads(json_str)\n",
    "        except json.JSONDecodeError as e:\n",
    "            logger.error(f\"JSON decoding error: {e}\")\n",
    "            return {}\n",
    "        \n",
    "def count_tokens(text: str) -> int:\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4.1-nano\")\n",
    "    tokens = encoding.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "def read_txt(file_path: str) -> str:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "022e45de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OpenAILLM:\n",
    "    def __init__(self, model: str=DEFAULT_LLM_MODEL, temperature: float=DEFAULT_TEMPERATURE, cache_path: str=CACHE_PATH):\n",
    "        self.client = OpenAI()\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        self.cache = self.load_cache(cache_path=cache_path)\n",
    "\n",
    "    def load_cache(self, cache_path: str) -> Dict[str, Any]:\n",
    "        Path(cache_path).mkdir(parents=True, exist_ok=True)\n",
    "        cache_file_path = os.path.join(cache_path, f\"{self.model.replace('/', '__')}.json\")\n",
    "        if os.path.exists(cache_file_path):\n",
    "            with open(cache_file_path, \"r\") as f:\n",
    "                cache_json = json.load(f)\n",
    "            cache_map = {}\n",
    "            for item in cache_json:\n",
    "                cache_map[item[\"prompt_hash\"]] = item\n",
    "            return cache_map\n",
    "        return {}\n",
    "    \n",
    "    def update_cache(self, prompt_hash: str, prompt: str, response: str, cache_path: str=CACHE_PATH):\n",
    "        self.cache[prompt_hash] = {\n",
    "            \"prompt_hash\": prompt_hash,\n",
    "            \"prompt\": prompt,\n",
    "            \"response\": response\n",
    "        }\n",
    "        cache_file_path = os.path.join(cache_path, f\"{self.model.replace('/', '__')}.json\")\n",
    "        Path(cache_path).mkdir(parents=True, exist_ok=True)\n",
    "        with open(cache_file_path, \"w\") as f:\n",
    "            json.dump(list(self.cache.values()), f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "    @retry.retry(tries=3, delay=10)\n",
    "    def _chat(self, messages: List[Dict[str, str]], force_overwrite_cache: bool=False) -> str:\n",
    "        try:\n",
    "            prompt_str = \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in messages])\n",
    "            prompt_hash = str(hash(prompt_str))\n",
    "            if not force_overwrite_cache and prompt_hash in self.cache:\n",
    "                logger.info(\"Cache hit for prompt.\")\n",
    "                return self.cache[prompt_hash].get(\"response\")\n",
    "            completion = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=messages,\n",
    "                temperature=self.temperature\n",
    "            )\n",
    "            response = completion.choices[0].message.content.strip()\n",
    "            self.update_cache(prompt_hash, prompt_str, response)\n",
    "            if not force_overwrite_cache:\n",
    "                logger.info(\"Cache missed, updated with new prompt.\")\n",
    "            else:\n",
    "                logger.info(\"Force overwrite cache, updated with new prompt.\")\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in OpenAI chat completion: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def chat(self, messages: List[Dict[str, str]], force_overwrite_cache: bool=False) -> str:\n",
    "        try:\n",
    "            if messages is None or len(messages) == 0:\n",
    "                raise ValueError(\"Messages cannot be empty.\")\n",
    "            return self._chat(messages, force_overwrite_cache)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error occurred in chat: {e}\")\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "c4991c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, llm: OpenAILLM=None):\n",
    "        self.llm = llm\n",
    "        \n",
    "    def get_rubric_features(self, session: dict) -> dict:\n",
    "        feats = {}\n",
    "        overall = session.get(\"assessment_data\",{}).get(\"overall\",{}).get(\"score\", np.nan)\n",
    "        feats[\"overall\"] = overall/100 if overall is not None else np.nan\n",
    "        criteria = session.get(\"assessment_data\",{}).get(\"criteria\",{})\n",
    "        for k in ACTIONS:\n",
    "            score = criteria.get(k,{}).get(\"score\", np.nan)\n",
    "            feats[k] = score/100 if score is not None else np.nan\n",
    "        return feats\n",
    "\n",
    "    def extract_baseline_features(self, session: dict) -> dict:\n",
    "        feats = self.get_rubric_features(session)\n",
    "\n",
    "        transcript = session.get(\"transcript\",[])\n",
    "        agent_messages = [t for t in transcript if \"agent_ai\" in t.get(\"type\",\"\").lower()]\n",
    "        user_messages = [t for t in transcript if \"user\" in t.get(\"type\",\"\").lower()]\n",
    "        agent_tokens = sum(count_tokens(t.get(\"data\",\"\")) for t in agent_messages)\n",
    "        user_tokens = sum(count_tokens(t.get(\"data\",\"\")) for t in user_messages)\n",
    "        feats[\"talk_ratio\"] = agent_tokens / (agent_tokens + user_tokens)\n",
    "\n",
    "        # latency\n",
    "        latencies = []\n",
    "        for i in range(len(transcript)-1):\n",
    "            t1 = transcript[i]\n",
    "            t2 = transcript[i+1]\n",
    "            if \"timestamp\" in t1 and \"timestamp\" in t2:\n",
    "                ts1 = parse_timestamp(t1[\"timestamp\"])\n",
    "                ts2 = parse_timestamp(t2[\"timestamp\"])\n",
    "                delta = (ts2 - ts1).total_seconds()\n",
    "                latencies.append(delta)\n",
    "            else:\n",
    "                logger.warning(f\"Missing timestamp in transcript entries: {t1}, {t2}\")\n",
    "\n",
    "        feats[\"latency\"] = np.mean(latencies) if latencies else 0.0\n",
    "        feats[\"duration\"] = float(session.get(\"duration\",0))\n",
    "        return feats\n",
    "    \n",
    "    def extract_llm_features(self, session: dict) -> dict:\n",
    "        transcript = session.get(\"transcript\",[])\n",
    "        system_prompt = read_txt(EXTRACT_LLM_FEATURES_SYSTEM_PROMPT_PATH)\n",
    "        transcript_str = parse_transcript_to_string(transcript)\n",
    "        user_prompt = f\"\"\"Here is the dialogue between an AI agent and a user:\n",
    "{transcript_str}\n",
    "\n",
    "Please provide your evaluation based on the criteria mentioned above in JSON format in JSON block (```json ... ```).\n",
    "\"\"\"\n",
    "        messages = [\n",
    "            {\"role\":\"system\", \"content\": system_prompt},\n",
    "            {\"role\":\"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "        response = self.llm.chat(messages, force_overwrite_cache=FORCE_OVERWRITE_CACHE)\n",
    "        try:\n",
    "            feats = extract_json(response)\n",
    "            if not feats:\n",
    "                raise ValueError(\"Empty JSON extracted\")\n",
    "            for k in feats:\n",
    "                feats[k] = feats[k]/100 if isinstance(feats[k], (int,float)) else 0.0\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to parse LLM response: {e}\")\n",
    "            feats = {\n",
    "                \"objection_monitoring\": 0.0,\n",
    "                \"cta_explicitness\": 0.0,\n",
    "                \"empathy_markers\": 0.0,\n",
    "                \"you_we_orientation\": 0.0,\n",
    "                \"collaborative_tone\": 0.0\n",
    "            }\n",
    "        question_count = self.get_question_ratio(session)\n",
    "        feats[\"question_ratio\"] = question_count\n",
    "        return feats\n",
    "    \n",
    "    def get_question_ratio(self, session: dict) -> dict:\n",
    "        transcript = session.get(\"transcript\",[])\n",
    "        agent_messages = [t for t in transcript if \"agent_ai\" in t.get(\"type\",\"\").lower()]\n",
    "        non_question_messages = [t for t in agent_messages if not t.get(\"data\",\"\").strip().endswith(\"?\")]\n",
    "        non_question_messages_str = \"\\n\".join(f\"{i+1}. {t.get('data','')}\" for i, t in enumerate(non_question_messages))\n",
    "        system_prompt = \"You are a helpful assistant.\"\n",
    "        user_prompt = f\"\"\"Here are some messages from an AI agent, these messages do NOT end with a question mark, please count how many of them are questions in nature (i.e. they are trying to elicit a response from the user):\n",
    "{non_question_messages_str}\n",
    "\n",
    "Please provide your answer as an integer number only.\n",
    "\"\"\"\n",
    "        messages = [\n",
    "            {\"role\":\"system\", \"content\": system_prompt},\n",
    "            {\"role\":\"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "        response = self.llm.chat(messages, force_overwrite_cache=FORCE_OVERWRITE_CACHE)\n",
    "        try:\n",
    "            question_count = int(response.strip())\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to parse question count from LLM response: {response}\")\n",
    "            question_count = 0\n",
    "        question_count += len(agent_messages) - len(non_question_messages)\n",
    "        total = len(agent_messages)\n",
    "        return question_count / total if total > 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "b71aa405",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CoachingCardGenerator:\n",
    "    def __init__(self, llm: OpenAILLM):\n",
    "        self.llm = llm\n",
    "\n",
    "    def generate(self, focus: str) -> dict:\n",
    "        system_prompt = read_txt(COACHING_CARD_GENERATION_SYSTEM_PROMPT_PATH)\n",
    "        user_prompt = f\"Given the focus skill: '{focus}'\\nPlease generate a coaching card as per the above criteria in JSON format in JSON block (```json ... ```).\"\n",
    "        messages = [\n",
    "            {\"role\":\"system\", \"content\": system_prompt},\n",
    "            {\"role\":\"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "        response = self.llm.chat(messages, force_overwrite_cache=FORCE_OVERWRITE_CACHE)\n",
    "        \n",
    "        try:\n",
    "            coaching_card = {\n",
    "                \"focus\": focus\n",
    "            }\n",
    "            coaching_card.update(extract_json(response))\n",
    "        except Exception:\n",
    "            logger.warning(f\"Failed to parse coaching card from LLM response. Use fallback response.\\nLLM response: {response}\")\n",
    "            coaching_card = {\n",
    "                \"focus\": focus,\n",
    "                \"why\": \"Practice this skill to improve performance.\",\n",
    "                \"exercises\": [\"Exercise 1\",\"Exercise 2\",\"Exercise 3\"],\n",
    "                \"scenario_stub\": {\n",
    "                    \"persona\": \"Generic friend\",\n",
    "                    \"opening\": \"Why should I try this?\",\n",
    "                    \"followups\": [\"Is it worth my time?\",\"Whatâ€™s fun about it?\"]\n",
    "                },\n",
    "                \"difficulty_upgrades\": [\n",
    "                    \"Increase pushback on objections.\",\n",
    "                    \"Reduce time to respond with clarity.\"\n",
    "                ]\n",
    "            }\n",
    "        return coaching_card\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "be14be28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AdaptiveLearningEngine:\n",
    "    def __init__(self, feature_extractor: FeatureExtractor, coaching_card_generator: CoachingCardGenerator, data_path: str=DATA_PATH):\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.coaching_card_generator = coaching_card_generator\n",
    "        self.data_path = data_path\n",
    "\n",
    "    def load_data(self, path=DATA_PATH):\n",
    "        with open(path,\"r\",encoding=\"utf-8-sig\") as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def compute_deltas(self, curr, nxt, skill: str):\n",
    "        a = curr.get(\"assessment_data\",{}).get(\"criteria\",{}).get(skill,{}).get(\"score\")\n",
    "        b = nxt.get(\"assessment_data\",{}).get(\"criteria\",{}).get(skill,{}).get(\"score\")\n",
    "        if a is not None and b is not None and not np.isnan(a) and not np.isnan(b):\n",
    "            return (b-a)/100.0\n",
    "        logger.warning(f\"Missing scores for skill '{skill}': curr={a}, next={b}\")\n",
    "        return 0.0\n",
    "\n",
    "    def run(self):\n",
    "        with open(self.data_path,\"r\",encoding=\"utf-8-sig\") as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        rows, baseline_vecs, full_vecs, labels = [], [], [], []\n",
    "\n",
    "        for i in range(len(data)-1):\n",
    "            curr, nxt = data[i], data[i+1]\n",
    "            session_id = curr.get(\"session_id\", str(i))\n",
    "            next_session_id = nxt.get(\"session_id\", str(i+1))\n",
    "\n",
    "            baseline_feats = self.feature_extractor.extract_baseline_features(curr)\n",
    "            llm_feats = self.feature_extractor.extract_llm_features(curr)\n",
    "            feats = {}\n",
    "            feats.update(baseline_feats)\n",
    "            feats.update(llm_feats)\n",
    "            rows.append({\"session_id\": session_id, **feats})\n",
    "\n",
    "            # baseline only vector\n",
    "            base_vec = np.array(list(baseline_feats.values()), dtype=float)\n",
    "            baseline_vecs.append(base_vec)\n",
    "\n",
    "            # full vector (baseline + llm features)\n",
    "            full_vec = np.array(list(feats.values()), dtype=float)\n",
    "            full_vecs.append(full_vec)\n",
    "\n",
    "            # delta overall\n",
    "            ov_curr = curr.get(\"assessment_data\",{}).get(\"overall\",{}).get(\"score\", np.nan)\n",
    "            ov_next = nxt.get(\"assessment_data\",{}).get(\"overall\",{}).get(\"score\", np.nan)\n",
    "            delta_overall = 0.0 if (np.isnan(ov_curr) or np.isnan(ov_next)) else (ov_next-ov_curr)/100.0\n",
    "\n",
    "            labels.append(1 if delta_overall>0 else 0)\n",
    "\n",
    "        # save features csv + json\n",
    "        pd.DataFrame(rows).to_csv(FEATURES_CSV_PATH,index=False)\n",
    "        with open(FEATURES_JSON_PATH, \"w\") as f:\n",
    "            json.dump(rows,f,indent=2,ensure_ascii=False)\n",
    "\n",
    "        X_base, X_full = np.vstack(baseline_vecs), np.vstack(full_vecs)\n",
    "        y = np.array(labels)     \n",
    "\n",
    "        loo = LeaveOneOut()\n",
    "        preds_base, preds_full, true = [], [], []\n",
    "        for train, test in loo.split(X_base):\n",
    "            # baseline\n",
    "            baseline_scaler = MinMaxScaler()\n",
    "            X_base[train] = baseline_scaler.fit_transform(X_base[train])\n",
    "            X_base[test] = baseline_scaler.transform(X_base[test])\n",
    "            clf_baseline = LogisticRegression(random_state=SEED)\n",
    "            clf_baseline.fit(X_base[train], y[train])\n",
    "            prob_b = clf_baseline.predict_proba(X_base[test])[:,1][0]\n",
    "            preds_base.append(prob_b)\n",
    "\n",
    "            # baseline+LLM\n",
    "            full_scaler = MinMaxScaler()\n",
    "            X_full[train] = full_scaler.fit_transform(X_full[train])\n",
    "            X_full[test] = full_scaler.transform(X_full[test])\n",
    "            clf_full = LogisticRegression(random_state=SEED)\n",
    "            clf_full.fit(X_full[train], y[train])\n",
    "            prob_f = clf_full.predict_proba(X_full[test])[:,1][0]\n",
    "            preds_full.append(prob_f)\n",
    "            true.append(y[test][0])\n",
    "\n",
    "        auc_b = roc_auc_score(true, preds_base) if len(set(true))>1 else float(\"nan\")\n",
    "        auc_f = roc_auc_score(true, preds_full) if len(set(true))>1 else float(\"nan\")\n",
    "\n",
    "        linucb = LinUCB(nchoices=len(ACTIONS), alpha=DEFAULT_LINUCB_ALPHA, random_state=SEED)\n",
    "        \n",
    "        rewards_rl = []\n",
    "        rewards_weak = []\n",
    "        chosen_actions_rl = []\n",
    "        coaching_cards = []\n",
    "        for i in range(len(data)-1):\n",
    "            curr, nxt = data[i], data[i+1]\n",
    "            session_id = curr.get(\"session_id\", str(i))\n",
    "            next_session_id = nxt.get(\"session_id\", str(i+1))\n",
    "            ov_curr = curr.get(\"assessment_data\",{}).get(\"overall\",{}).get(\"score\", np.nan)\n",
    "            ov_next = nxt.get(\"assessment_data\",{}).get(\"overall\",{}).get(\"score\", np.nan)\n",
    "            delta_overall = 0.0 if (np.isnan(ov_curr) or np.isnan(ov_next)) else (ov_next-ov_curr)/100.0\n",
    "            \n",
    "            baseline_feats = self.feature_extractor.extract_baseline_features(curr)\n",
    "            llm_feats = self.feature_extractor.extract_llm_features(curr)\n",
    "            feats = {}\n",
    "            feats.update(baseline_feats)\n",
    "            feats.update(llm_feats)\n",
    "            full_vec = np.array(list(feats.values()), dtype=float)\n",
    "            \n",
    "            # choose next action rl\n",
    "            chosen_action_rl_index = linucb.predict(full_vec.reshape(1, -1))[0]\n",
    "            chosen_action_rl = ACTIONS[chosen_action_rl_index]\n",
    "            chosen_actions_rl.append(chosen_action_rl)\n",
    "            delta_skill_rl = self.compute_deltas(curr,nxt, chosen_action_rl)\n",
    "            rewards_rl.append(DEFAULT_SKILL_WEIGHT*delta_skill_rl + DEFAULT_OVERALL_WEIGHT*delta_overall)\n",
    "\n",
    "            # generate coaching card\n",
    "            coaching_card = self.coaching_card_generator.generate(chosen_action_rl)\n",
    "            coaching_cards.append({\"step\": i, \"session_id\": session_id, \"next_session_id\": next_session_id, **coaching_card})\n",
    "            \n",
    "            # choose next action weak\n",
    "            chosen_action_weak_index = np.argmin([baseline_feats.get(a,0.0) for a in ACTIONS])\n",
    "            chosen_action_weak = ACTIONS[chosen_action_weak_index]\n",
    "            delta_skill_weak = self.compute_deltas(curr,nxt, chosen_action_weak)\n",
    "            rewards_weak.append(DEFAULT_SKILL_WEIGHT*delta_skill_weak + DEFAULT_OVERALL_WEIGHT*delta_overall)\n",
    "            \n",
    "            # train linucb for next step with all actions\n",
    "            X_full = np.tile(full_vec, (len(ACTIONS),1))\n",
    "\n",
    "            actions = []\n",
    "            actions_reward = []\n",
    "            for action_index, action in enumerate(ACTIONS):\n",
    "                actions.append(action_index)\n",
    "                delta_skill = self.compute_deltas(curr,nxt, action)\n",
    "                actions_reward.append(DEFAULT_SKILL_WEIGHT*delta_skill + DEFAULT_OVERALL_WEIGHT*delta_overall)\n",
    "\n",
    "            actions = np.array(actions)\n",
    "            actions_reward = np.array(actions_reward)\n",
    "            # print(X_full.shape, actions.shape, actions_reward.shape)\n",
    "            linucb.partial_fit(X_full, actions, actions_reward)\n",
    "    \n",
    "        avg_lin, avg_weak = np.mean(rewards_rl), np.mean(rewards_weak)\n",
    "            \n",
    "        with open(COACHING_CARDS_JSON,\"w\") as f: \n",
    "            json.dump(coaching_cards,f,indent=2)\n",
    "\n",
    "        report = {\n",
    "            \"sessions\": len(data),\n",
    "            \"steps\": len(data)-1,\n",
    "            \"policy\": {\n",
    "                \"linucb_alpha\": DEFAULT_LINUCB_ALPHA,\n",
    "                \"skill_weight\": DEFAULT_SKILL_WEIGHT,\n",
    "                \"overall_weight\": DEFAULT_OVERALL_WEIGHT\n",
    "            },\n",
    "            \"results\": {\n",
    "                \"rl_linucb_mean_reward\": float(avg_lin),\n",
    "                \"weakest_mean_reward\": float(avg_weak)\n",
    "            },\n",
    "            \"feature_ablation\": {\n",
    "                \"baseline_auc\": float(auc_b), \"full_auc\": float(auc_f)\n",
    "            }\n",
    "        }\n",
    "        with open(REPORT_JSON_PATH,\"w\") as f: \n",
    "            json.dump(report,f,indent=2)\n",
    "\n",
    "        print(\"-------- REPORT ---------\")\n",
    "        print(json.dumps(report, indent=2, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "f5b579f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n",
      "INFO:__main__:Cache hit for prompt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- REPORT ---------\n",
      "{\n",
      "  \"sessions\": 19,\n",
      "  \"steps\": 18,\n",
      "  \"policy\": {\n",
      "    \"linucb_alpha\": 0.4,\n",
      "    \"skill_weight\": 0.6,\n",
      "    \"overall_weight\": 0.4\n",
      "  },\n",
      "  \"results\": {\n",
      "    \"rl_linucb_mean_reward\": -0.02061111111111111,\n",
      "    \"weakest_mean_reward\": 0.026388888888888882\n",
      "  },\n",
      "  \"feature_ablation\": {\n",
      "    \"baseline_auc\": 0.3538461538461538,\n",
      "    \"full_auc\": 0.5076923076923077\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__==\"__main__\":\n",
    "    preprocess_dataset(DATA_PATH, PREPROCESSED_DATA_PATH)\n",
    "    llm = OpenAILLM(model=DEFAULT_LLM_MODEL, temperature=DEFAULT_TEMPERATURE)\n",
    "    feature_extractor = FeatureExtractor(llm)\n",
    "    coaching_card_generator = CoachingCardGenerator(llm)\n",
    "    engine = AdaptiveLearningEngine(feature_extractor, coaching_card_generator, PREPROCESSED_DATA_PATH)\n",
    "    engine.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59a949b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adaptive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
